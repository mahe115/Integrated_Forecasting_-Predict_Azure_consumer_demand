{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d2f5050",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TFTTrainer' from 'pytorch_forecasting.models.temporal_fusion_transformer' (d:\\infosysspringboard projects\\project1-1stmilestine\\azure_analytics\\lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GroupNormalizer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuantileLoss\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtemporal_fusion_transformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TFTTrainer\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NaNLabelEncoder\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Step 1: Load dataset\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TFTTrainer' from 'pytorch_forecasting.models.temporal_fusion_transformer' (d:\\infosysspringboard projects\\project1-1stmilestine\\azure_analytics\\lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer import TFTTrainer\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "\n",
    "# Step 1: Load dataset\n",
    "df = pd.read_csv(\"D:\\infosysspringboard projects\\project1-1stmilestine\\AZURE_BACKEND_TEAM-B\\data\\processed\\cleaned_merged.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Step 2: Sort and create time_idx\n",
    "df = df.sort_values(by=[\"region\", \"resource_type\", \"date\"]).reset_index(drop=True)\n",
    "df[\"time_idx\"] = df.groupby([\"region\", \"resource_type\"]).cumcount()\n",
    "\n",
    "# Step 3: Define necessary columns\n",
    "max_prediction_length = 7    # 7 days to predict\n",
    "max_encoder_length = 30      # use past 30 days\n",
    "target = \"usage_cpu\"\n",
    "\n",
    "# Step 4: Rename categorical columns as needed, encode if necessary\n",
    "# Convert categorical columns properly\n",
    "df[\"region\"] = df[\"region\"].astype(\"category\")\n",
    "df[\"resource_type\"] = df[\"resource_type\"].astype(\"category\")\n",
    "\n",
    "# Convert holiday explicitly to string category to avoid numeric type error\n",
    "df[\"holiday\"] = df[\"holiday\"].astype(str).astype(\"category\")\n",
    "\n",
    "\n",
    "# Step 5: Define training dataset for TFT\n",
    "training_cutoff = df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df[df.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target,\n",
    "    group_ids=[\"region\", \"resource_type\"],\n",
    "    min_encoder_length=max_encoder_length,  # keep encoder length consistent\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"region\", \"resource_type\"],\n",
    "    time_varying_known_categoricals=[\"holiday\"],\n",
    "    time_varying_known_reals=[\"time_idx\", \"economic_index\", \"cloud_market_demand\", \"users_active\"],\n",
    "    time_varying_unknown_reals=[target],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"region\", \"resource_type\"], transformation=\"softplus\"\n",
    "    ),  # helps stabilize training\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# Step 6: Create validation dataset\n",
    "validation = TimeSeriesDataSet.from_dataset(training, df, predict=True, stop_randomization=True)\n",
    "\n",
    "# Step 7: Create dataloaders\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "# Step 8: Define the model\n",
    "trainer = torch.optim.Adam\n",
    "early_stop_callback = None  # Can add early stopping for more epochs if needed\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # 7 quantiles by default for quantile regression\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# Step 9: Train the model\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "pl_trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "pl_trainer.fit(\n",
    "    tft,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "\n",
    "# Step 10: Save the model checkpoint\n",
    "save_path = \"tft_cpu_usage_model.ckpt\"\n",
    "pl_trainer.save_checkpoint(save_path)\n",
    "\n",
    "# Step 11: Load the trained model for inference\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(save_path, dataset=training)\n",
    "\n",
    "# Step 12: Make predictions on validation data\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "\n",
    "# Step 13: Visualize actual vs predicted for a sample series\n",
    "def plot_prediction(x, raw_predictions, idx=0):\n",
    "    import matplotlib.pyplot as plt\n",
    "    actual = x[\"decoder_target\"][idx].detach().cpu().numpy()\n",
    "    prediction = raw_predictions['prediction'][idx].detach().cpu().numpy()\n",
    "\n",
    "    encoder_length = x['encoder_lengths'][idx].item()\n",
    "    predict_length = prediction.shape[0]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(encoder_length), x['encoder_target'][idx].detach().cpu().numpy(), label=\"History (usage_cpu)\")\n",
    "    plt.plot(range(encoder_length, encoder_length + predict_length), actual, label=\"Actual (usage_cpu)\")\n",
    "    plt.plot(range(encoder_length, encoder_length + predict_length), prediction, label=\"Predicted (usage_cpu)\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time index\")\n",
    "    plt.ylabel(\"CPU Usage\")\n",
    "    plt.title(f\"Temporal Fusion Transformer Forecasting Usage CPU (Sample {idx})\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot for the first sample\n",
    "plot_prediction(x, raw_predictions, idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f351702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Preprocessing data...\n",
      "Training dataset created with 1068 samples\n",
      "Validation dataset created with 12 samples\n",
      "Dataloaders created - Train batches: 16, Val batches: 1\n",
      "Creating TFT model...\n",
      "Model created with 81165 parameters\n",
      "Training model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 141\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    133\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m    134\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     enable_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    139\u001b[0m )\n\u001b[1;32m--> 141\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Step 11: Save the model\u001b[39;00m\n\u001b[0;32m    148\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtft_cpu_usage_model.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\infosysspringboard projects\\project1-1stmilestine\\azure_analytics\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:553\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    509\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     ckpt_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    551\u001b[0m \n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_unwrap_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    555\u001b[0m     _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n",
      "File \u001b[1;32md:\\infosysspringboard projects\\project1-1stmilestine\\azure_analytics\\lib\\site-packages\\pytorch_lightning\\utilities\\compile.py:111\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    110\u001b[0m _check_mixed_imports(model)\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Step 1: Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(r\"D:\\infosysspringboard projects\\project1-1stmilestine\\AZURE_BACKEND_TEAM-B\\data\\processed\\cleaned_merged.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Step 2: Preprocessing\n",
    "print(\"Preprocessing data...\")\n",
    "df = df.sort_values(by=[\"region\", \"resource_type\", \"date\"]).reset_index(drop=True)\n",
    "df[\"time_idx\"] = df.groupby([\"region\", \"resource_type\"]).cumcount()\n",
    "\n",
    "# Ensure target column is float\n",
    "df[\"usage_cpu\"] = df[\"usage_cpu\"].astype(float)\n",
    "\n",
    "# Convert categories\n",
    "df[\"region\"] = df[\"region\"].astype(\"category\")\n",
    "df[\"resource_type\"] = df[\"resource_type\"].astype(\"category\")\n",
    "df[\"holiday\"] = df[\"holiday\"].astype(str).astype(\"category\")\n",
    "df[\"group_id\"] = df[\"region\"].astype(str) + \"_\" + df[\"resource_type\"].astype(str)\n",
    "\n",
    "# Define cutoff for training (example: last 7 days as validation)\n",
    "training_cutoff = df[\"time_idx\"].max() - 7\n",
    "\n",
    "# Prepare training dataframe and ensure float\n",
    "train_df = df[df.time_idx <= training_cutoff].copy()\n",
    "train_df[\"usage_cpu\"] = train_df[\"usage_cpu\"].astype(float)\n",
    "\n",
    "max_prediction_length = 7\n",
    "max_encoder_length = 30\n",
    "target = \"usage_cpu\"\n",
    "\n",
    "# Create TimeSeriesDataSet for training\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target,\n",
    "    group_ids=[\"group_id\"],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"region\", \"resource_type\"],\n",
    "    time_varying_known_categoricals=[\"holiday\"],\n",
    "    time_varying_known_reals=[\"time_idx\", \"economic_index\", \"cloud_market_demand\", \"users_active\"],\n",
    "    time_varying_unknown_reals=[target],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"group_id\"], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Training dataset created with {len(training)} samples\")\n",
    "\n",
    "# Create validation dataset\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,\n",
    "    df,\n",
    "    predict=True,\n",
    "    stop_randomization=True\n",
    ")\n",
    "\n",
    "print(f\"Validation dataset created with {len(validation)} samples\")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 2, num_workers=0)\n",
    "\n",
    "print(f\"Dataloaders created - Train batches: {len(train_dataloader)}, Val batches: {len(val_dataloader)}\")\n",
    "\n",
    "# Callbacks\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "\n",
    "# Create TFT model from dataset\n",
    "print(\"Creating TFT model...\")\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "    output_size=7,  # quantiles\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    "    optimizer=\"Adam\"\n",
    ")\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in tft.parameters())} parameters\")\n",
    "\n",
    "# Trainer initialization\n",
    "print(\"Training model...\")\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if not torch.cuda.is_available() else \"auto\",  # fix device param\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],\n",
    "    enable_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Fit model - pass exact model instance tft\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "\n",
    "# Save best model checkpoint\n",
    "save_path = checkpoint_callback.best_model_path if checkpoint_callback.best_model_path else \"tft_cpu_usage_model.ckpt\"\n",
    "print(f\"Best model path: {save_path}\")\n",
    "\n",
    "# Load best model for inference\n",
    "if os.path.exists(save_path):\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(save_path)\n",
    "else:\n",
    "    best_tft = tft\n",
    "\n",
    "print(\"Best model loaded for inference\")\n",
    "\n",
    "# Make predictions on validation data\n",
    "print(\"Making predictions...\")\n",
    "raw_predictions = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "predictions, x = raw_predictions if isinstance(raw_predictions, tuple) else (raw_predictions, next(iter(val_dataloader)))\n",
    "\n",
    "print(\"Predictions completed\")\n",
    "\n",
    "# Plotting function omitted for brevity - keep as is\n",
    "\n",
    "# Metrics function omitted for brevity - keep as is\n",
    "\n",
    "# Generate visualization and metrics\n",
    "print(\"\\nGenerating visualization and metrics...\")\n",
    "plot_predictions_comparison(x, predictions)\n",
    "metrics = calculate_metrics(x, predictions)\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nAnalyzing feature importance...\")\n",
    "interpretation = best_tft.interpret_output(raw_predictions, reduction=\"sum\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "if 'encoder_variables' in interpretation:\n",
    "    encoder_importance = interpretation['encoder_variables']\n",
    "    axes[0].barh(range(len(encoder_importance)), encoder_importance)\n",
    "    axes[0].set_title('Encoder Variable Importance')\n",
    "    axes[0].set_xlabel('Importance Score')\n",
    "if 'decoder_variables' in interpretation:\n",
    "    decoder_importance = interpretation['decoder_variables']\n",
    "    axes[1].barh(range(len(decoder_importance)), decoder_importance)\n",
    "    axes[1].set_title('Decoder Variable Importance')\n",
    "    axes[1].set_xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.savefig('tft_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TFT MODEL TRAINING AND EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ Model saved to: {save_path}\")\n",
    "print(f\"âœ“ Predictions visualization saved to: tft_predictions_comparison.png\")\n",
    "print(f\"âœ“ Feature importance plot saved to: tft_feature_importance.png\")\n",
    "print(f\"âœ“ Training completed with RMSE: {metrics['RMSE']:.2f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save predictions CSV\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': x[\"decoder_target\"].flatten().detach().cpu().numpy(),\n",
    "    'predicted': predictions.prediction.flatten().detach().cpu().numpy() if hasattr(predictions, 'prediction') else predictions.flatten().detach().cpu().numpy()\n",
    "})\n",
    "predictions_df.to_csv('tft_predictions.csv', index=False)\n",
    "print(f\"âœ“ Predictions saved to: tft_predictions.csv\")\n",
    "\n",
    "print(\"\\nTraining and evaluation pipeline completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4742efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Preprocessing data...\n",
      "Training dataset created with 1068 samples\n",
      "Validation dataset created with 12 samples\n",
      "Dataloaders created - Train batches: 16, Val batches: 1\n",
      "Creating TFT model...\n",
      "Model created with 81165 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 117\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Step 9: Train the model\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Step 10: Load the best model for inference\u001b[39;00m\n\u001b[0;32m    120\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m checkpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path\n",
      "File \u001b[1;32md:\\infosysspringboard projects\\project1-1stmilestine\\azure_analytics\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:553\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    509\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     ckpt_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    551\u001b[0m \n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_unwrap_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    555\u001b[0m     _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n",
      "File \u001b[1;32md:\\infosysspringboard projects\\project1-1stmilestine\\azure_analytics\\lib\\site-packages\\pytorch_lightning\\utilities\\compile.py:111\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    110\u001b[0m _check_mixed_imports(model)\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Step 1: Load dataset - use raw string for Windows path\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(r\"D:\\infosysspringboard projects\\project1-1stmilestine\\AZURE_BACKEND_TEAM-B\\data\\processed\\cleaned_merged.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Step 2: Preprocessing\n",
    "print(\"Preprocessing data...\")\n",
    "df = df.sort_values(by=[\"region\", \"resource_type\", \"date\"]).reset_index(drop=True)\n",
    "df[\"time_idx\"] = df.groupby([\"region\", \"resource_type\"]).cumcount()\n",
    "\n",
    "# Ensure usage_cpu target is float\n",
    "df[\"usage_cpu\"] = df[\"usage_cpu\"].astype(float)\n",
    "\n",
    "# Convert relevant columns to categorical\n",
    "df[\"region\"] = df[\"region\"].astype(\"category\")\n",
    "df[\"resource_type\"] = df[\"resource_type\"].astype(\"category\")\n",
    "df[\"holiday\"] = df[\"holiday\"].astype(str).astype(\"category\")\n",
    "df[\"group_id\"] = df[\"region\"].astype(str) + \"_\" + df[\"resource_type\"].astype(str)\n",
    "\n",
    "# Define cutoff for training (keeping last 7 days for validation)\n",
    "max_prediction_length = 7\n",
    "max_encoder_length = 30\n",
    "target = \"usage_cpu\"\n",
    "training_cutoff = df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "# Training dataframe\n",
    "train_df = df[df.time_idx <= training_cutoff].copy()\n",
    "\n",
    "# Step 3: Create TimeSeriesDataSet for training\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target,\n",
    "    group_ids=[\"group_id\"],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"region\", \"resource_type\"],\n",
    "    time_varying_known_categoricals=[\"holiday\"],\n",
    "    time_varying_known_reals=[\"time_idx\", \"economic_index\", \"cloud_market_demand\", \"users_active\"],\n",
    "    time_varying_unknown_reals=[target],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"group_id\"], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Training dataset created with {len(training)} samples\")\n",
    "\n",
    "# Step 4: Create validation dataset using entire data for prediction\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,\n",
    "    df,\n",
    "    predict=True,\n",
    "    stop_randomization=True,\n",
    ")\n",
    "\n",
    "print(f\"Validation dataset created with {len(validation)} samples\")\n",
    "\n",
    "# Step 5: Create dataloaders\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 2, num_workers=0)\n",
    "\n",
    "print(f\"Dataloaders created - Train batches: {len(train_dataloader)}, Val batches: {len(val_dataloader)}\")\n",
    "\n",
    "# Step 6: Define callbacks for training\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, mode=\"min\", verbose=True)\n",
    "lr_logger = LearningRateMonitor()\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, filename=\"best-tft\")\n",
    "\n",
    "# Step 7: Define the TFT model\n",
    "print(\"Creating TFT model...\")\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    "    optimizer=\"Adam\",\n",
    ")\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in tft.parameters())} parameters\")\n",
    "\n",
    "# Step 8: Initialize trainer with callbacks and correct device setting\n",
    "devices = 1 if not torch.cuda.is_available() else \"auto\"\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\",\n",
    "    devices=devices,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop_callback, lr_logger, checkpoint_callback],\n",
    "    enable_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Step 9: Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "\n",
    "# Step 10: Load the best model for inference\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "if best_model_path and os.path.exists(best_model_path):\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "    print(f\"Loaded best model checkpoint: {best_model_path}\")\n",
    "else:\n",
    "    best_tft = tft\n",
    "    print(\"Best model checkpoint not found, using last model.\")\n",
    "\n",
    "# Step 11: Make predictions on validation set\n",
    "print(\"Making predictions...\")\n",
    "raw_predictions = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "predictions, x = raw_predictions if isinstance(raw_predictions, tuple) else (raw_predictions, next(iter(val_dataloader)))\n",
    "\n",
    "print(\"Prediction completed\")\n",
    "\n",
    "# Step 12: Plot actual vs predicted for multiple samples\n",
    "def plot_predictions_comparison(x, predictions, idx_list=[0, 1, 2, 3]):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, idx in enumerate(idx_list[:4]):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "        ax = axes[i]\n",
    "        prediction = predictions.prediction[idx].detach().cpu().numpy() if hasattr(predictions, 'prediction') else predictions[idx].detach().cpu().numpy()\n",
    "        actual = x[\"decoder_target\"][idx].detach().cpu().numpy()\n",
    "        encoder_target = x[\"encoder_target\"][idx].detach().cpu().numpy()\n",
    "        encoder_length = x['encoder_lengths'][idx].item()\n",
    "        history_time = range(encoder_length)\n",
    "        future_time = range(encoder_length, encoder_length + len(actual))\n",
    "        ax.plot(history_time, encoder_target, 'b-', label=\"Historical CPU Usage\", linewidth=2)\n",
    "        ax.plot(future_time, actual, 'g-', label=\"Actual CPU Usage\", linewidth=2, marker='o')\n",
    "        ax.plot(future_time, prediction, 'r--', label=\"Predicted CPU Usage\", linewidth=2, marker='s')\n",
    "        ax.set_xlabel(\"Time Steps\")\n",
    "        ax.set_ylabel(\"CPU Usage\")\n",
    "        ax.set_title(f\"TFT Forecast - Sample {idx}\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tft_predictions_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Step 13: Calculate evaluation metrics\n",
    "def calculate_metrics(x, predictions):\n",
    "    pred_values = predictions.prediction.detach().cpu().numpy() if hasattr(predictions, 'prediction') else predictions.detach().cpu().numpy()\n",
    "    actual_values = x[\"decoder_target\"].detach().cpu().numpy()\n",
    "    mae = np.mean(np.abs(pred_values - actual_values))\n",
    "    mse = np.mean((pred_values - actual_values) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((actual_values - pred_values) / actual_values)) * 100\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL EVALUATION METRICS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "    print(\"=\"*50)\n",
    "    return {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"MAPE\": mape}\n",
    "\n",
    "# Step 14: Generate plots and calculate metrics\n",
    "print(\"\\nGenerating visualization and metrics...\")\n",
    "plot_predictions_comparison(x, predictions)\n",
    "metrics = calculate_metrics(x, predictions)\n",
    "\n",
    "# Step 15: Feature importance analysis and visualization\n",
    "print(\"\\nAnalyzing feature importance...\")\n",
    "interpretation = best_tft.interpret_output(raw_predictions, reduction=\"sum\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "if 'encoder_variables' in interpretation:\n",
    "    encoder_importance = interpretation['encoder_variables']\n",
    "    axes[0].barh(range(len(encoder_importance)), encoder_importance)\n",
    "    axes[0].set_title('Encoder Variable Importance')\n",
    "    axes[0].set_xlabel('Importance Score')\n",
    "if 'decoder_variables' in interpretation:\n",
    "    decoder_importance = interpretation['decoder_variables']\n",
    "    axes[1].barh(range(len(decoder_importance)), decoder_importance)\n",
    "    axes[1].set_title('Decoder Variable Importance')\n",
    "    axes[1].set_xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.savefig('tft_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TFT MODEL TRAINING AND EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ Model saved to: {best_model_path}\")\n",
    "print(f\"âœ“ Predictions visualization saved to: tft_predictions_comparison.png\")\n",
    "print(f\"âœ“ Feature importance plot saved to: tft_feature_importance.png\")\n",
    "print(f\"âœ“ Training completed with RMSE: {metrics['RMSE']:.2f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 16: Save predictions to CSV for further analysis\n",
    "pred_df = pd.DataFrame({\n",
    "    'actual': x[\"decoder_target\"].flatten().detach().cpu().numpy(),\n",
    "    'predicted': predictions.prediction.flatten().detach().cpu().numpy() if hasattr(predictions, 'prediction') else predictions.flatten().detach().cpu().numpy()\n",
    "})\n",
    "pred_df.to_csv('tft_predictions.csv', index=False)\n",
    "print(\"âœ“ Predictions saved to: tft_predictions.csv\")\n",
    "\n",
    "print(\"\\nTraining and evaluation pipeline completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebce4be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
