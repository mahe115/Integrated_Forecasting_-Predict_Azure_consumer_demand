{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Azure Cloud Resource Forecasting Analysis ===\n",
      "Analysis started at: 2025-09-22 14:32:57\n",
      "============================================================\n",
      "Error: Data file 'cleaned_merged.csv' not found!\n",
      "Please update the 'data_file' variable with the correct path.\n"
     ]
    }
   ],
   "source": [
    "# Complete Improved Code for Time Series Forecasting Models\n",
    "# Fixed all issues and optimized for CPU Usage prediction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# ML and Time Series Libraries\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Deep Learning Libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"=== Azure Cloud Resource Forecasting Analysis ===\")\n",
    "print(f\"Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==========================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"Calculate RMSE, MAE, and MAPE for model evaluation\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # Handle MAPE calculation to avoid division by zero\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true != 0, y_true, 1))) * 100\n",
    "    \n",
    "    print(f\"{model_name} Metrics:\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return rmse, mae, mape\n",
    "\n",
    "def plot_predictions_with_dates(dates, y_true, y_pred, title, save_path=None):\n",
    "    \"\"\"Enhanced plotting function with dates on x-axis\"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Ensure dates are in datetime format\n",
    "    if not isinstance(dates, pd.DatetimeIndex):\n",
    "        dates = pd.to_datetime(dates)\n",
    "    \n",
    "    plt.plot(dates, y_true, label='Actual CPU Usage', color='#2E86AB', linewidth=2.5, alpha=0.8)\n",
    "    plt.plot(dates, y_pred, label='Predicted CPU Usage', color='#F24236', linestyle='--', linewidth=2.5, alpha=0.9)\n",
    "    \n",
    "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Date', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('CPU Usage (%)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12, loc='best')\n",
    "    plt.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)\n",
    "    plt.xticks(rotation=45, fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    \n",
    "    # Add some styling\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"  Plot saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def create_directory(path):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# ==========================================\n",
    "# DATA PREPROCESSING\n",
    "# ==========================================\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"Load and prepare the dataset with feature engineering\"\"\"\n",
    "    print(\"1. Loading and preparing data...\")\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"   Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Convert date and sort\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Add derived features for CPU and Storage efficiency\n",
    "    print(\"   Adding derived features...\")\n",
    "    max_cpu_per_resource = df.groupby('resource_type')['usage_cpu'].transform('max')\n",
    "    df['cpu_utilization'] = df['usage_cpu'] / max_cpu_per_resource\n",
    "    \n",
    "    max_storage_per_resource = df.groupby('resource_type')['usage_storage'].transform('max')\n",
    "    df['storage_efficiency'] = df['usage_storage'] / max_storage_per_resource\n",
    "    \n",
    "    # Add time-based features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    df_encoded = pd.get_dummies(df, columns=['region', 'resource_type'], prefix=['region', 'resource_type'])\n",
    "    \n",
    "    print(f\"   Final dataset shape: {df_encoded.shape}\")\n",
    "    return df_encoded, df\n",
    "\n",
    "# ==========================================\n",
    "# ARIMA MODEL\n",
    "# ==========================================\n",
    "\n",
    "def prepare_arima_data(df):\n",
    "    \"\"\"Prepare data specifically for ARIMA model (univariate time series)\"\"\"\n",
    "    print(\"\\n2. Preparing ARIMA dataset...\")\n",
    "    \n",
    "    # Aggregate CPU usage by date (average across all regions and resource types)\n",
    "    arima_df = df.groupby('date').agg({\n",
    "        'usage_cpu': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Set date as index for time series analysis\n",
    "    arima_df.set_index('date', inplace=True)\n",
    "    \n",
    "    print(f\"   ARIMA dataset shape: {arima_df.shape}\")\n",
    "    return arima_df\n",
    "\n",
    "def train_arima_model(arima_df, test_size=0.2):\n",
    "    \"\"\"Train and evaluate ARIMA model\"\"\"\n",
    "    print(\"\\n3. Training ARIMA Model...\")\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(len(arima_df) * (1 - test_size))\n",
    "    train_arima = arima_df[:train_size]\n",
    "    test_arima = arima_df[train_size:]\n",
    "    \n",
    "    print(f\"   Train size: {len(train_arima)}, Test size: {len(test_arima)}\")\n",
    "    \n",
    "    try:\n",
    "        # Fit ARIMA model with optimized parameters\n",
    "        model_arima = sm.tsa.ARIMA(train_arima['usage_cpu'], order=(2,1,2))\n",
    "        model_arima_fit = model_arima.fit()\n",
    "        \n",
    "        # Forecast\n",
    "        forecast_arima = model_arima_fit.forecast(steps=len(test_arima))\n",
    "        forecast_arima.index = test_arima.index\n",
    "        \n",
    "        # Evaluate\n",
    "        rmse, mae, mape = evaluate_metrics(test_arima['usage_cpu'], forecast_arima, \"ARIMA\")\n",
    "        \n",
    "        # Plot\n",
    "        plot_predictions_with_dates(\n",
    "            test_arima.index, \n",
    "            test_arima['usage_cpu'], \n",
    "            forecast_arima, \n",
    "            'ARIMA Model: Actual vs Predicted CPU Usage'\n",
    "        )\n",
    "        \n",
    "        return model_arima_fit, forecast_arima, test_arima, (rmse, mae, mape)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ARIMA model failed: {str(e)}\")\n",
    "        return None, None, None, (None, None, None)\n",
    "\n",
    "# ==========================================\n",
    "# XGBOOST MODEL\n",
    "# ==========================================\n",
    "\n",
    "def train_xgboost_model(df_encoded, test_size=0.2):\n",
    "    \"\"\"Train and evaluate XGBoost model\"\"\"\n",
    "    print(\"\\n4. Training XGBoost Model...\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    feature_cols = [col for col in df_encoded.columns if col not in ['date', 'usage_cpu']]\n",
    "    \n",
    "    X = df_encoded[feature_cols]\n",
    "    y = df_encoded['usage_cpu']\n",
    "    dates = df_encoded['date']\n",
    "    \n",
    "    # Split data (temporal split)\n",
    "    train_size = int(len(df_encoded) * (1 - test_size))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    test_dates = dates[train_size:]\n",
    "    \n",
    "    print(f\"   Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "    print(f\"   Number of features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    model_xgb = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model_xgb.fit(X_train, y_train)\n",
    "    preds_xgb = model_xgb.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    rmse, mae, mape = evaluate_metrics(y_test, preds_xgb, \"XGBoost\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model_xgb.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n   Top 10 Most Important Features:\")\n",
    "    for idx, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"     {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Plot\n",
    "    plot_predictions_with_dates(\n",
    "        test_dates.reset_index(drop=True), \n",
    "        y_test.reset_index(drop=True), \n",
    "        preds_xgb, \n",
    "        'XGBoost Model: Actual vs Predicted CPU Usage'\n",
    "    )\n",
    "    \n",
    "    return model_xgb, preds_xgb, y_test, test_dates, (rmse, mae, mape), feature_importance\n",
    "\n",
    "# ==========================================\n",
    "# LSTM MODEL\n",
    "# ==========================================\n",
    "\n",
    "def create_lstm_sequences(data, feature_cols, target_col, dates, seq_length=7):\n",
    "    \"\"\"Create sequences for LSTM model\"\"\"\n",
    "    X, y, sequence_dates = [], [], []\n",
    "    \n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[feature_cols].iloc[i:i+seq_length].values)\n",
    "        y.append(data[target_col].iloc[i+seq_length])\n",
    "        sequence_dates.append(dates.iloc[i+seq_length])\n",
    "    \n",
    "    return np.array(X), np.array(y), sequence_dates\n",
    "\n",
    "def train_lstm_model(df_encoded, test_size=0.2, seq_length=7):\n",
    "    \"\"\"Train and evaluate LSTM model\"\"\"\n",
    "    print(\"\\n5. Training LSTM Model...\")\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_cols = [col for col in df_encoded.columns if col not in ['date', 'usage_cpu']]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = df_encoded.copy()\n",
    "    df_scaled[feature_cols + ['usage_cpu']] = scaler.fit_transform(df_scaled[feature_cols + ['usage_cpu']])\n",
    "    \n",
    "    # Create sequences\n",
    "    X_lstm, y_lstm, sequence_dates = create_lstm_sequences(\n",
    "        df_scaled, feature_cols, 'usage_cpu', df_encoded['date'], seq_length\n",
    "    )\n",
    "    \n",
    "    print(f\"   Sequence shape: {X_lstm.shape}\")\n",
    "    print(f\"   Number of sequences: {len(X_lstm)}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(len(X_lstm) * (1 - test_size))\n",
    "    X_train_lstm = X_lstm[:train_size]\n",
    "    X_test_lstm = X_lstm[train_size:]\n",
    "    y_train_lstm = y_lstm[:train_size]\n",
    "    y_test_lstm = y_lstm[train_size:]\n",
    "    test_dates_lstm = sequence_dates[train_size:]\n",
    "    \n",
    "    print(f\"   Train sequences: {len(X_train_lstm)}, Test sequences: {len(X_test_lstm)}\")\n",
    "    \n",
    "    # Build LSTM model\n",
    "    model_lstm = Sequential([\n",
    "        LSTM(64, activation='relu', return_sequences=True, input_shape=(seq_length, len(feature_cols))),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train model\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    history = model_lstm.fit(\n",
    "        X_train_lstm, y_train_lstm,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    preds_lstm_scaled = model_lstm.predict(X_test_lstm, verbose=0).flatten()\n",
    "    \n",
    "    # Inverse scale predictions and actual values\n",
    "    y_test_original = scaler.inverse_transform(\n",
    "        np.column_stack([np.zeros((len(y_test_lstm), len(feature_cols))), y_test_lstm])\n",
    "    )[:, -1]\n",
    "    \n",
    "    preds_lstm_original = scaler.inverse_transform(\n",
    "        np.column_stack([np.zeros((len(preds_lstm_scaled), len(feature_cols))), preds_lstm_scaled])\n",
    "    )[:, -1]\n",
    "    \n",
    "    # Evaluate\n",
    "    rmse, mae, mape = evaluate_metrics(y_test_original, preds_lstm_original, \"LSTM\")\n",
    "    \n",
    "    # Plot\n",
    "    plot_predictions_with_dates(\n",
    "        test_dates_lstm, \n",
    "        y_test_original, \n",
    "        preds_lstm_original, \n",
    "        'LSTM Model: Actual vs Predicted CPU Usage'\n",
    "    )\n",
    "    \n",
    "    return model_lstm, preds_lstm_original, y_test_original, test_dates_lstm, (rmse, mae, mape), history\n",
    "\n",
    "# ==========================================\n",
    "# MODEL COMPARISON\n",
    "# ==========================================\n",
    "\n",
    "def compare_models(arima_metrics, xgb_metrics, lstm_metrics):\n",
    "    \"\"\"Compare all three models\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    models = ['ARIMA', 'XGBoost', 'LSTM']\n",
    "    metrics = [arima_metrics, xgb_metrics, lstm_metrics]\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': models,\n",
    "        'RMSE': [m[0] if m[0] is not None else np.nan for m in metrics],\n",
    "        'MAE': [m[1] if m[1] is not None else np.nan for m in metrics],\n",
    "        'MAPE': [m[2] if m[2] is not None else np.nan for m in metrics]\n",
    "    })\n",
    "    \n",
    "    print(comparison_df.to_string(index=False, float_format='{:.4f}'.format))\n",
    "    \n",
    "    # Find best model for each metric\n",
    "    if not comparison_df['RMSE'].isna().all():\n",
    "        best_rmse = comparison_df.loc[comparison_df['RMSE'].idxmin(), 'Model']\n",
    "        best_mae = comparison_df.loc[comparison_df['MAE'].idxmin(), 'Model']\n",
    "        best_mape = comparison_df.loc[comparison_df['MAPE'].idxmin(), 'Model']\n",
    "        \n",
    "        print(f\"\\nBest Models:\")\n",
    "        print(f\"  RMSE: {best_rmse}\")\n",
    "        print(f\"  MAE:  {best_mae}\")\n",
    "        print(f\"  MAPE: {best_mape}\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# ==========================================\n",
    "# MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    # File paths - Update these paths according to your directory structure\n",
    "    data_file = 'D:/infosysspringboard projects/project1-1stmilestine/AZURE_BACKEND_TEAM-B/data/processed/cleaned_merged.csv'  # Update this path\n",
    "    \n",
    "    # Check if file exists``\n",
    "    if not os.path.exists(data_file):\n",
    "        print(f\"Error: Data file '{data_file}' not found!\")\n",
    "        print(\"Please update the 'data_file' variable with the correct path.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Load and prepare data\n",
    "        df_encoded, df_original = load_and_prepare_data(data_file)\n",
    "        \n",
    "        # Train ARIMA model\n",
    "        arima_df = prepare_arima_data(df_original)\n",
    "        arima_model, arima_forecast, arima_test, arima_metrics = train_arima_model(arima_df)\n",
    "        \n",
    "        # Train XGBoost model\n",
    "        xgb_model, xgb_preds, xgb_test, xgb_dates, xgb_metrics, xgb_importance = train_xgboost_model(df_encoded)\n",
    "        \n",
    "        # Train LSTM model\n",
    "        lstm_model, lstm_preds, lstm_test, lstm_dates, lstm_metrics, lstm_history = train_lstm_model(df_encoded)\n",
    "        \n",
    "        # Compare models\n",
    "        comparison_df = compare_models(arima_metrics, xgb_metrics, lstm_metrics)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Return results for further use\n",
    "        return {\n",
    "            'arima': {'model': arima_model, 'metrics': arima_metrics},\n",
    "            'xgboost': {'model': xgb_model, 'metrics': xgb_metrics, 'feature_importance': xgb_importance},\n",
    "            'lstm': {'model': lstm_model, 'metrics': lstm_metrics, 'history': lstm_history},\n",
    "            'comparison': comparison_df\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during execution: {str(e)}\")\n",
    "        print(\"Please check your data file path and ensure all required libraries are installed.\")\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# USAGE INSTRUCTIONS\n",
    "# ==========================================\n",
    "\n",
    "\"\"\"\n",
    "USAGE INSTRUCTIONS:\n",
    "1. Update the 'data_file' path in the main() function\n",
    "2. Ensure you have the following libraries installed:\n",
    "   - pandas, numpy, matplotlib, seaborn\n",
    "   - scikit-learn, xgboost\n",
    "   - statsmodels\n",
    "   - tensorflow\n",
    "3. Run the script: results = main()\n",
    "\n",
    "The script will:\n",
    "- Load and preprocess your data\n",
    "- Train ARIMA, XGBoost, and LSTM models\n",
    "- Evaluate each model with RMSE, MAE, MAPE\n",
    "- Create visualizations comparing actual vs predicted values\n",
    "- Provide a summary comparison of all models\n",
    "\n",
    "For Jupyter Notebook usage:\n",
    "- Copy individual functions as needed\n",
    "- Run main() to execute the complete analysis\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188dc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
