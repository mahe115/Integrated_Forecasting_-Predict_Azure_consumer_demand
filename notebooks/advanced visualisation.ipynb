{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Azure Demand Forecasting - Additional Insightful Visualizations\n",
    "## Comprehensive Analysis Beyond the Basic Data Cleaning Notebook\n",
    "\n",
    "This notebook contains additional visualizations that provide deeper insights into the Azure usage patterns, including seasonal trends, regional comparisons, resource utilization patterns, and external factor impacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'AZURE_BACKEND_TEAM-B\\\\data\\\\processed\\\\cleaned_merged.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the cleaned merged data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAZURE_BACKEND_TEAM-B\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mprocessed\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcleaned_merged.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create additional time-based features\u001b[39;00m\n",
      "File \u001b[1;32md:\\infosysspringboard projects\\project1-1stmilestine\\azure_analytics\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\infosysspringboard projects\\project1-1stmilestine\\azure_analytics\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\infosysspringboard projects\\project1-1stmilestine\\azure_analytics\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\infosysspringboard projects\\project1-1stmilestine\\azure_analytics\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\infosysspringboard projects\\project1-1stmilestine\\azure_analytics\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AZURE_BACKEND_TEAM-B\\\\data\\\\processed\\\\cleaned_merged.csv'"
     ]
    }
   ],
   "source": [
    "# Load the cleaned merged data\n",
    "df = pd.read_csv('AZURE_BACKEND_TEAM-B/data/processed/cleaned_merged.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Create additional time-based features\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day_of_week'] = df['date'].dt.day_of_week\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Create month names and day names for better visualization\n",
    "month_names = {1: 'Jan', 2: 'Feb', 3: 'Mar'}\n",
    "day_names = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'}\n",
    "df['month_name'] = df['month'].map(month_names)\n",
    "df['day_name'] = df['day_of_week'].map(day_names)\n",
    "\n",
    "print(f\"Dataset loaded with {df.shape[0]} records from {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"Regions: {df['region'].unique()}\")\n",
    "print(f\"Resource types: {df['resource_type'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Analysis - Multi-Metric Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive time series dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle('Azure Usage Time Series Analysis - Multi-Metric Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Daily aggregated data\n",
    "daily_data = df.groupby('date').agg({\n",
    "    'usage_cpu': 'mean',\n",
    "    'usage_storage': 'mean',\n",
    "    'users_active': 'mean',\n",
    "    'economic_index': 'first',\n",
    "    'cloud_market_demand': 'first',\n",
    "    'holiday': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# 1. CPU Usage over time with holiday markers\n",
    "axes[0,0].plot(daily_data['date'], daily_data['usage_cpu'], linewidth=2, color='#2E86AB', alpha=0.8)\n",
    "holiday_dates = daily_data[daily_data['holiday'] == 1]['date']\n",
    "holiday_cpu = daily_data[daily_data['holiday'] == 1]['usage_cpu']\n",
    "axes[0,0].scatter(holiday_dates, holiday_cpu, color='red', s=50, alpha=0.7, zorder=5, label='Holidays')\n",
    "axes[0,0].set_title('Average CPU Usage Over Time', fontweight='bold')\n",
    "axes[0,0].set_ylabel('CPU Usage (%)')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Storage usage trends\n",
    "axes[0,1].plot(daily_data['date'], daily_data['usage_storage'], linewidth=2, color='#A23B72', alpha=0.8)\n",
    "axes[0,1].fill_between(daily_data['date'], daily_data['usage_storage'], alpha=0.3, color='#A23B72')\n",
    "axes[0,1].set_title('Average Storage Usage Over Time', fontweight='bold')\n",
    "axes[0,1].set_ylabel('Storage Usage (GB)')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Active users trend\n",
    "axes[1,0].plot(daily_data['date'], daily_data['users_active'], linewidth=2, color='#F18F01', alpha=0.8)\n",
    "axes[1,0].set_title('Active Users Over Time', fontweight='bold')\n",
    "axes[1,0].set_ylabel('Active Users')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Economic index vs Cloud market demand\n",
    "ax1 = axes[1,1]\n",
    "ax2 = ax1.twinx()\n",
    "line1 = ax1.plot(daily_data['date'], daily_data['economic_index'], color='#C73E1D', linewidth=2, label='Economic Index')\n",
    "line2 = ax2.plot(daily_data['date'], daily_data['cloud_market_demand'], color='#5D737E', linewidth=2, label='Cloud Market Demand')\n",
    "ax1.set_ylabel('Economic Index', color='#C73E1D')\n",
    "ax2.set_ylabel('Cloud Market Demand', color='#5D737E')\n",
    "ax1.set_title('Economic Indicators Over Time', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regional Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional comparison analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Regional Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Average CPU usage by region\n",
    "regional_stats = df.groupby('region').agg({\n",
    "    'usage_cpu': ['mean', 'std'],\n",
    "    'usage_storage': ['mean', 'std'],\n",
    "    'users_active': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "regional_stats.columns = ['_'.join(col).strip() for col in regional_stats.columns]\n",
    "\n",
    "# CPU usage by region with error bars\n",
    "regions = regional_stats.index\n",
    "cpu_means = regional_stats['usage_cpu_mean']\n",
    "cpu_stds = regional_stats['usage_cpu_std']\n",
    "\n",
    "bars1 = axes[0,0].bar(regions, cpu_means, yerr=cpu_stds, capsize=5, \n",
    "                     color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'], alpha=0.8)\n",
    "axes[0,0].set_title('Average CPU Usage by Region', fontweight='bold')\n",
    "axes[0,0].set_ylabel('CPU Usage (%)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars1, cpu_means):\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                  f'{val:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Storage usage by region\n",
    "storage_means = regional_stats['usage_storage_mean']\n",
    "storage_stds = regional_stats['usage_storage_std']\n",
    "\n",
    "bars2 = axes[0,1].bar(regions, storage_means, yerr=storage_stds, capsize=5,\n",
    "                     color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'], alpha=0.8)\n",
    "axes[0,1].set_title('Average Storage Usage by Region', fontweight='bold')\n",
    "axes[0,1].set_ylabel('Storage Usage (GB)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, val in zip(bars2, storage_means):\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "                  f'{val:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Regional usage distribution (violin plot)\n",
    "sns.violinplot(data=df, x='region', y='usage_cpu', ax=axes[1,0], palette='Set2')\n",
    "axes[1,0].set_title('CPU Usage Distribution by Region', fontweight='bold')\n",
    "axes[1,0].set_ylabel('CPU Usage (%)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Regional correlation heatmap\n",
    "region_pivot = df.groupby('region')[['usage_cpu', 'usage_storage', 'users_active']].mean()\n",
    "correlation_matrix = region_pivot.T.corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0, \n",
    "           square=True, ax=axes[1,1], cbar_kws={'shrink': 0.8})\n",
    "axes[1,1].set_title('Regional Performance Correlation', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resource Type Analysis and Optimization Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource type analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Resource Type Analysis and Optimization Insights', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Resource utilization efficiency (CPU vs Storage)\n",
    "resource_colors = {'VM': '#E74C3C', 'Storage': '#3498DB', 'Container': '#2ECC71'}\n",
    "for resource in df['resource_type'].unique():\n",
    "    subset = df[df['resource_type'] == resource]\n",
    "    axes[0,0].scatter(subset['usage_cpu'], subset['usage_storage'], \n",
    "                     alpha=0.6, s=30, label=resource, color=resource_colors[resource])\n",
    "\n",
    "axes[0,0].set_xlabel('CPU Usage (%)')\n",
    "axes[0,0].set_ylabel('Storage Usage (GB)')\n",
    "axes[0,0].set_title('Resource Utilization Patterns', fontweight='bold')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Resource type performance over time\n",
    "resource_daily = df.groupby(['date', 'resource_type'])['usage_cpu'].mean().reset_index()\n",
    "for resource in df['resource_type'].unique():\n",
    "    subset = resource_daily[resource_daily['resource_type'] == resource]\n",
    "    axes[0,1].plot(subset['date'], subset['usage_cpu'], \n",
    "                  linewidth=2, label=resource, color=resource_colors[resource], alpha=0.8)\n",
    "\n",
    "axes[0,1].set_title('Resource Type Performance Trends', fontweight='bold')\n",
    "axes[0,1].set_ylabel('Average CPU Usage (%)')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Resource efficiency matrix (heatmap)\n",
    "efficiency_matrix = df.groupby(['region', 'resource_type'])['usage_cpu'].mean().unstack()\n",
    "sns.heatmap(efficiency_matrix, annot=True, fmt='.1f', cmap='YlOrRd', \n",
    "           ax=axes[1,0], cbar_kws={'label': 'Average CPU Usage (%)'})\n",
    "axes[1,0].set_title('Resource Efficiency by Region', fontweight='bold')\n",
    "axes[1,0].set_xlabel('Resource Type')\n",
    "axes[1,0].set_ylabel('Region')\n",
    "\n",
    "# 4. User activity vs resource usage\n",
    "df_sample = df.sample(n=500)  # Sample for better visualization\n",
    "scatter = axes[1,1].scatter(df_sample['users_active'], df_sample['usage_cpu'], \n",
    "                           c=df_sample['usage_storage'], s=50, alpha=0.6, \n",
    "                           cmap='viridis', edgecolors='black', linewidth=0.5)\n",
    "\n",
    "axes[1,1].set_xlabel('Active Users')\n",
    "axes[1,1].set_ylabel('CPU Usage (%)')\n",
    "axes[1,1].set_title('User Activity vs Resource Usage\\n(Color = Storage Usage)', fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=axes[1,1])\n",
    "cbar.set_label('Storage Usage (GB)', rotation=270, labelpad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Seasonal and Temporal Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal pattern analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Seasonal and Temporal Pattern Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Monthly trend analysis\n",
    "monthly_data = df.groupby('month_name').agg({\n",
    "    'usage_cpu': 'mean',\n",
    "    'usage_storage': 'mean',\n",
    "    'users_active': 'mean'\n",
    "})\n",
    "\n",
    "months_order = ['Jan', 'Feb', 'Mar']\n",
    "monthly_ordered = monthly_data.reindex(months_order)\n",
    "\n",
    "x_pos = np.arange(len(months_order))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = axes[0,0].bar(x_pos - width, monthly_ordered['usage_cpu'], width, \n",
    "                     label='CPU Usage', color='#FF6B6B', alpha=0.8)\n",
    "bars2 = axes[0,0].bar(x_pos, monthly_ordered['usage_storage']/20, width, \n",
    "                     label='Storage Usage/20', color='#4ECDC4', alpha=0.8)\n",
    "bars3 = axes[0,0].bar(x_pos + width, monthly_ordered['users_active']/5, width, \n",
    "                     label='Users Active/5', color='#45B7D1', alpha=0.8)\n",
    "\n",
    "axes[0,0].set_xlabel('Month')\n",
    "axes[0,0].set_ylabel('Normalized Usage')\n",
    "axes[0,0].set_title('Monthly Usage Patterns', fontweight='bold')\n",
    "axes[0,0].set_xticks(x_pos)\n",
    "axes[0,0].set_xticklabels(months_order)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. Day of week analysis\n",
    "dow_data = df.groupby('day_name').agg({\n",
    "    'usage_cpu': 'mean',\n",
    "    'usage_storage': 'mean'\n",
    "})\n",
    "\n",
    "day_order = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "dow_ordered = dow_data.reindex(day_order)\n",
    "\n",
    "axes[0,1].plot(day_order, dow_ordered['usage_cpu'], marker='o', linewidth=3, \n",
    "              markersize=8, color='#E74C3C', label='CPU Usage')\n",
    "ax_twin = axes[0,1].twinx()\n",
    "ax_twin.plot(day_order, dow_ordered['usage_storage'], marker='s', linewidth=3, \n",
    "            markersize=8, color='#3498DB', label='Storage Usage')\n",
    "\n",
    "axes[0,1].set_xlabel('Day of Week')\n",
    "axes[0,1].set_ylabel('CPU Usage (%)', color='#E74C3C')\n",
    "ax_twin.set_ylabel('Storage Usage (GB)', color='#3498DB')\n",
    "axes[0,1].set_title('Weekly Usage Patterns', fontweight='bold')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Weekend vs Weekday analysis\n",
    "weekend_analysis = df.groupby(['is_weekend', 'resource_type'])['usage_cpu'].mean().unstack()\n",
    "weekend_analysis.index = ['Weekday', 'Weekend']\n",
    "\n",
    "weekend_analysis.plot(kind='bar', ax=axes[0,2], width=0.7, alpha=0.8,\n",
    "                     color=['#E74C3C', '#3498DB', '#2ECC71'])\n",
    "axes[0,2].set_title('Weekend vs Weekday Usage', fontweight='bold')\n",
    "axes[0,2].set_ylabel('Average CPU Usage (%)')\n",
    "axes[0,2].set_xlabel('Day Type')\n",
    "axes[0,2].tick_params(axis='x', rotation=0)\n",
    "axes[0,2].legend(title='Resource Type')\n",
    "\n",
    "# 4. Holiday impact analysis\n",
    "holiday_impact = df.groupby(['holiday', 'region']).agg({\n",
    "    'usage_cpu': 'mean',\n",
    "    'users_active': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "holiday_impact['holiday_label'] = holiday_impact['holiday'].map({0: 'Regular Day', 1: 'Holiday'})\n",
    "\n",
    "sns.boxplot(data=df, x='holiday', y='usage_cpu', hue='region', ax=axes[1,0])\n",
    "axes[1,0].set_title('Holiday Impact on CPU Usage by Region', fontweight='bold')\n",
    "axes[1,0].set_xlabel('Holiday (0=No, 1=Yes)')\n",
    "axes[1,0].set_ylabel('CPU Usage (%)')\n",
    "\n",
    "# 5. Economic index correlation with usage\n",
    "economic_corr = df.groupby(pd.cut(df['economic_index'], bins=5)).agg({\n",
    "    'usage_cpu': 'mean',\n",
    "    'usage_storage': 'mean',\n",
    "    'users_active': 'mean'\n",
    "})\n",
    "\n",
    "# Create labels for economic index ranges\n",
    "econ_labels = [f'{int(interval.left)}-{int(interval.right)}' for interval in economic_corr.index]\n",
    "\n",
    "x_pos = np.arange(len(econ_labels))\n",
    "axes[1,1].bar(x_pos, economic_corr['usage_cpu'], alpha=0.7, color='#9B59B6')\n",
    "axes[1,1].set_xlabel('Economic Index Range')\n",
    "axes[1,1].set_ylabel('Average CPU Usage (%)')\n",
    "axes[1,1].set_title('Economic Impact on Usage', fontweight='bold')\n",
    "axes[1,1].set_xticks(x_pos)\n",
    "axes[1,1].set_xticklabels(econ_labels, rotation=45)\n",
    "\n",
    "# 6. Cloud market demand vs usage correlation\n",
    "demand_bins = pd.cut(df['cloud_market_demand'], bins=5)\n",
    "demand_corr = df.groupby(demand_bins)['usage_cpu'].mean()\n",
    "\n",
    "demand_labels = [f'{interval.left:.2f}-{interval.right:.2f}' for interval in demand_corr.index]\n",
    "x_pos = np.arange(len(demand_labels))\n",
    "axes[1,2].bar(x_pos, demand_corr.values, alpha=0.7, color='#F39C12')\n",
    "axes[1,2].set_xlabel('Cloud Market Demand Range')\n",
    "axes[1,2].set_ylabel('Average CPU Usage (%)')\n",
    "axes[1,2].set_title('Market Demand vs Usage', fontweight='bold')\n",
    "axes[1,2].set_xticks(x_pos)\n",
    "axes[1,2].set_xticklabels(demand_labels, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Statistical Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced statistical analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Advanced Statistical Analysis and Insights', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Correlation heatmap of all numerical features\n",
    "numeric_cols = ['usage_cpu', 'usage_storage', 'users_active', 'economic_index', \n",
    "               'cloud_market_demand', 'holiday', 'is_weekend']\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "           square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, ax=axes[0,0])\n",
    "axes[0,0].set_title('Feature Correlation Matrix', fontweight='bold')\n",
    "\n",
    "# 2. Distribution analysis of key metrics\n",
    "key_metrics = ['usage_cpu', 'usage_storage', 'users_active']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(key_metrics, colors)):\n",
    "    axes[0,1].hist(df[metric], bins=30, alpha=0.6, label=metric.replace('_', ' ').title(), \n",
    "                  color=color, density=True)\n",
    "\n",
    "axes[0,1].set_xlabel('Normalized Value')\n",
    "axes[0,1].set_ylabel('Density')\n",
    "axes[0,1].set_title('Distribution of Key Metrics', fontweight='bold')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# 3. Resource efficiency analysis\n",
    "df['cpu_per_user'] = df['usage_cpu'] / df['users_active']\n",
    "df['storage_per_user'] = df['usage_storage'] / df['users_active']\n",
    "\n",
    "efficiency_data = df.groupby(['region', 'resource_type']).agg({\n",
    "    'cpu_per_user': 'mean',\n",
    "    'storage_per_user': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Create bubble chart\n",
    "for region in df['region'].unique():\n",
    "    region_data = efficiency_data[efficiency_data['region'] == region]\n",
    "    axes[1,0].scatter(region_data['cpu_per_user'], region_data['storage_per_user'], \n",
    "                     s=200, alpha=0.7, label=region)\n",
    "\n",
    "axes[1,0].set_xlabel('CPU Usage per User')\n",
    "axes[1,0].set_ylabel('Storage Usage per User')\n",
    "axes[1,0].set_title('Resource Efficiency by Region', fontweight='bold')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Peak usage identification\n",
    "daily_peaks = df.groupby('date').agg({\n",
    "    'usage_cpu': ['mean', 'max', 'min'],\n",
    "    'usage_storage': ['mean', 'max', 'min']\n",
    "})\n",
    "\n",
    "# Flatten column names\n",
    "daily_peaks.columns = ['_'.join(col).strip() for col in daily_peaks.columns]\n",
    "\n",
    "axes[1,1].fill_between(daily_peaks.index, daily_peaks['usage_cpu_min'], \n",
    "                      daily_peaks['usage_cpu_max'], alpha=0.3, color='#E74C3C', label='CPU Range')\n",
    "axes[1,1].plot(daily_peaks.index, daily_peaks['usage_cpu_mean'], \n",
    "              linewidth=2, color='#E74C3C', label='CPU Average')\n",
    "\n",
    "# Add secondary axis for storage\n",
    "ax_twin = axes[1,1].twinx()\n",
    "ax_twin.fill_between(daily_peaks.index, daily_peaks['usage_storage_min'], \n",
    "                    daily_peaks['usage_storage_max'], alpha=0.3, color='#3498DB')\n",
    "ax_twin.plot(daily_peaks.index, daily_peaks['usage_storage_mean'], \n",
    "            linewidth=2, color='#3498DB', label='Storage Average')\n",
    "\n",
    "axes[1,1].set_xlabel('Date')\n",
    "axes[1,1].set_ylabel('CPU Usage (%)', color='#E74C3C')\n",
    "ax_twin.set_ylabel('Storage Usage (GB)', color='#3498DB')\n",
    "axes[1,1].set_title('Daily Usage Ranges and Peaks', fontweight='bold')\n",
    "axes[1,1].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary statistics\n",
    "print(\"üîç AZURE DEMAND FORECASTING - PERFORMANCE INSIGHTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\nüìä OVERALL PERFORMANCE METRICS:\")\n",
    "print(f\"‚Ä¢ Average CPU Usage: {df['usage_cpu'].mean():.1f}% (¬±{df['usage_cpu'].std():.1f}%)\")\n",
    "print(f\"‚Ä¢ Average Storage Usage: {df['usage_storage'].mean():.0f} GB (¬±{df['usage_storage'].std():.0f} GB)\")\n",
    "print(f\"‚Ä¢ Average Active Users: {df['users_active'].mean():.0f} (¬±{df['users_active'].std():.0f})\")\n",
    "print(f\"‚Ä¢ Peak CPU Usage: {df['usage_cpu'].max()}%\")\n",
    "print(f\"‚Ä¢ Peak Storage Usage: {df['usage_storage'].max()} GB\")\n",
    "\n",
    "# Regional insights\n",
    "print(\"\\nüåç REGIONAL PERFORMANCE:\")\n",
    "regional_summary = df.groupby('region').agg({\n",
    "    'usage_cpu': 'mean',\n",
    "    'usage_storage': 'mean',\n",
    "    'users_active': 'mean'\n",
    "}).round(1)\n",
    "\n",
    "for region in regional_summary.index:\n",
    "    cpu_avg = regional_summary.loc[region, 'usage_cpu']\n",
    "    storage_avg = regional_summary.loc[region, 'usage_storage']\n",
    "    users_avg = regional_summary.loc[region, 'users_active']\n",
    "    print(f\"‚Ä¢ {region}: CPU {cpu_avg}%, Storage {storage_avg:.0f}GB, Users {users_avg:.0f}\")\n",
    "\n",
    "# Resource type insights\n",
    "print(\"\\n‚öôÔ∏è RESOURCE TYPE ANALYSIS:\")\n",
    "resource_summary = df.groupby('resource_type').agg({\n",
    "    'usage_cpu': 'mean',\n",
    "    'usage_storage': 'mean'\n",
    "}).round(1)\n",
    "\n",
    "for resource in resource_summary.index:\n",
    "    cpu_avg = resource_summary.loc[resource, 'usage_cpu']\n",
    "    storage_avg = resource_summary.loc[resource, 'usage_storage']\n",
    "    print(f\"‚Ä¢ {resource}: CPU {cpu_avg}%, Storage {storage_avg:.0f}GB\")\n",
    "\n",
    "# Temporal insights\n",
    "print(\"\\n‚è∞ TEMPORAL PATTERNS:\")\n",
    "holiday_effect = df.groupby('holiday')['usage_cpu'].mean()\n",
    "weekend_effect = df.groupby('is_weekend')['usage_cpu'].mean()\n",
    "\n",
    "print(f\"‚Ä¢ Holiday Effect: Regular days {holiday_effect[0]:.1f}% vs Holidays {holiday_effect[1]:.1f}%\")\n",
    "print(f\"‚Ä¢ Weekend Effect: Weekdays {weekend_effect[0]:.1f}% vs Weekends {weekend_effect[1]:.1f}%\")\n",
    "\n",
    "# Correlation insights\n",
    "print(\"\\nüîó KEY CORRELATIONS:\")\n",
    "cpu_storage_corr = df['usage_cpu'].corr(df['usage_storage'])\n",
    "cpu_users_corr = df['usage_cpu'].corr(df['users_active'])\n",
    "cpu_economic_corr = df['usage_cpu'].corr(df['economic_index'])\n",
    "\n",
    "print(f\"‚Ä¢ CPU vs Storage Usage: {cpu_storage_corr:.3f}\")\n",
    "print(f\"‚Ä¢ CPU vs Active Users: {cpu_users_corr:.3f}\")\n",
    "print(f\"‚Ä¢ CPU vs Economic Index: {cpu_economic_corr:.3f}\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nüí° KEY RECOMMENDATIONS:\")\n",
    "best_region = regional_summary['usage_cpu'].idxmax()\n",
    "most_efficient_resource = resource_summary['usage_cpu'].idxmin()\n",
    "\n",
    "print(f\"‚Ä¢ Highest performing region: {best_region}\")\n",
    "print(f\"‚Ä¢ Most CPU-efficient resource type: {most_efficient_resource}\")\n",
    "print(f\"‚Ä¢ Holiday planning: Usage {'increases' if holiday_effect[1] > holiday_effect[0] else 'decreases'} during holidays\")\n",
    "print(f\"‚Ä¢ Weekend optimization: Usage {'increases' if weekend_effect[1] > weekend_effect[0] else 'decreases'} on weekends\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary dataframes for export\n",
    "\n",
    "# 1. Daily summary statistics\n",
    "daily_summary = df.groupby('date').agg({\n",
    "    'usage_cpu': ['mean', 'max', 'min', 'std'],\n",
    "    'usage_storage': ['mean', 'max', 'min', 'std'],\n",
    "    'users_active': ['mean', 'max', 'min', 'std'],\n",
    "    'economic_index': 'first',\n",
    "    'cloud_market_demand': 'first',\n",
    "    'holiday': 'first'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "daily_summary.columns = ['_'.join(col).strip() if col[1] else col[0] for col in daily_summary.columns]\n",
    "\n",
    "# 2. Regional performance summary\n",
    "regional_performance = df.groupby(['region', 'resource_type']).agg({\n",
    "    'usage_cpu': ['mean', 'std'],\n",
    "    'usage_storage': ['mean', 'std'],\n",
    "    'users_active': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "regional_performance.columns = ['_'.join(col).strip() for col in regional_performance.columns]\n",
    "\n",
    "# 3. Correlation analysis\n",
    "correlation_analysis = df[['usage_cpu', 'usage_storage', 'users_active', \n",
    "                          'economic_index', 'cloud_market_demand']].corr().round(3)\n",
    "\n",
    "# Save to CSV files\n",
    "daily_summary.to_csv('azure_daily_summary.csv')\n",
    "regional_performance.to_csv('azure_regional_performance.csv')\n",
    "correlation_analysis.to_csv('azure_correlation_analysis.csv')\n",
    "\n",
    "print(\"üìÅ Summary files exported:\")\n",
    "print(\"‚Ä¢ azure_daily_summary.csv - Daily aggregated statistics\")\n",
    "print(\"‚Ä¢ azure_regional_performance.csv - Regional and resource type analysis\")\n",
    "print(\"‚Ä¢ azure_correlation_analysis.csv - Feature correlation matrix\")\n",
    "\n",
    "# Display sample of exported data\n",
    "print(\"\\nüìã Sample of Daily Summary:\")\n",
    "print(daily_summary.head())\n",
    "\n",
    "print(\"\\nüìã Sample of Regional Performance:\")\n",
    "print(regional_performance.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
